{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vbipin/aip/blob/master/mdp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pr2dA9HmkePh"
   },
   "outputs": [],
   "source": [
    "#we plan to implement some of the algorithms related to MDPs and RL\n",
    "#MDP study\n",
    "#%matplotlib inline\n",
    "#import matplotlib\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#I am trying to avoid the numpy dependencies\n",
    "\n",
    "import random\n",
    "#\n",
    "#We plan to implement the gridworld class \n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NU8O0Do5k-8S"
   },
   "outputs": [],
   "source": [
    "#Let us have a gridworld\n",
    "#ref: Chapter 17, Artificial Intelligence a Modern Approach\n",
    "#ref: CS188 https://inst.eecs.berkeley.edu/~cs188/fa19/\n",
    "#ref: https://inst.eecs.berkeley.edu/~cs188/fa19/assets/slides/lec8.pdf\n",
    "#ref: https://courses.cs.washington.edu/courses/cse473/13au/slides/17-mdp-rl.pdf\n",
    "\n",
    "#This class will create a 2D grid of row x colums \n",
    "#Some of the cells can be disabled by putting it into walls\n",
    "#cells are addressed just like 2d arrays (r,c)\n",
    "#There are possibly many terminal states\n",
    "#terminal states have only one action available: Exit \n",
    "#Transistion is as per the book 80% action and 20%sideways ( a variable noise is used to control this distribution)\n",
    "#There is a special end state, (-1,-1), from which NO action is available. This state is used as a final state.\n",
    "\n",
    "#Actions #just some alias\n",
    "Up    = 0\n",
    "Down  = 1\n",
    "Right = 2\n",
    "Left  = 3\n",
    "Exit  = 4\n",
    "\n",
    "class GridWorld :\n",
    "    #Default is as given in the AIMA book\n",
    "    def __init__(self, \n",
    "                 rows    =3, \n",
    "                 columns =4, \n",
    "                 walls   =[(1,1)], terminals= {(0,3):+1.0, (1,3):-1.0}, \n",
    "                 gamma   =1.0, \n",
    "                 living_reward=0,\n",
    "                 noise   =0.2\n",
    "                ) :\n",
    "        \"\"\"We dont expect these parameters to change during the agent run\"\"\"\n",
    "        self.rows      = rows\n",
    "        self.columns   = columns\n",
    "        self.N         = rows * columns #total cells\n",
    "        self.walls     = walls\n",
    "        self.terminals = terminals #dictionary of terminal celss and their rewards.\n",
    "        self.gamma     = gamma\n",
    "        self.living_reward = living_reward\n",
    "        self.all_actions   = [ Up, Down, Right, Left, Exit ]\n",
    "        self.end_state     = (-1, -1) #a dummy state to reach after taking Exit\n",
    "        self.all_states    = [(r,c) for r in range(rows) for c in range(columns) if (r,c) not in walls ] + [self.end_state]\n",
    "        self.noise         = noise\n",
    "        \n",
    "        \n",
    "        #transitions from each state and the probabilities\n",
    "        self.noise                = noise\n",
    "        self.action_transitions   = { \n",
    "            Up:   ([Up,    Left, Right], [1-noise, noise/2, noise/2 ]),\n",
    "            Down: ([Down,  Left, Right], [1-noise, noise/2, noise/2 ]),\n",
    "            Left: ([Left,  Up,   Down ], [1-noise, noise/2, noise/2 ]),\n",
    "            Right:([Right, Up,   Down ], [1-noise, noise/2, noise/2 ]),\n",
    "            Exit :([Exit], [1.0])\n",
    "        }\n",
    "    \n",
    "    def actions(self, state) :\n",
    "        \"\"\"returns all valid actions from the current state\"\"\"\n",
    "        if state in self.terminals :\n",
    "            return [Exit]\n",
    "        if state == self.end_state :\n",
    "            return [] #No action available.\n",
    "        return [ Up, Down, Right, Left ]\n",
    "    \n",
    "    def reward(self, state, action, next_state=None) :\n",
    "        \"\"\"reward is the instantaneous reward. It is usually R(s,a,s')\"\"\"\n",
    "        #In grid world the reward depends only on state.\n",
    "        if state in self.terminals :\n",
    "            return self.terminals[state] #dict has the terminal values +1 or -1\n",
    "        if state == self.end_state :\n",
    "            return 0.0\n",
    "        return self.living_reward        #usually a small -ve value\n",
    "    \n",
    "    def transitions(self, state, action) :\n",
    "        \"\"\"returna list of tuple(nextstate, action, probability)\"\"\"\n",
    "        actual_actions, probs = self.action_transitions[action]\n",
    "        return [ self._next_cell(state, a) for a in actual_actions ], actual_actions, probs\n",
    "    \n",
    "    def move(self, state, action) :\n",
    "        \"\"\"Take the action and return the tuple(new_state, reward, is_terminal)\"\"\"                          \n",
    "        assert action in self.actions(state) #just a check if this is a valid action at this time or not\n",
    "        \n",
    "        cells, actions, p = self.transitions(state, action)\n",
    "        \n",
    "        #we choose one cell acccording to probabilities\n",
    "        new_state   = random.choices(cells, weights=p)[0] #only one; we take index 0                \n",
    "        reward      = self.reward(state, action) #\n",
    "        \n",
    "        is_terminal = False\n",
    "        if new_state == self.end_state :\n",
    "            is_terminal = True\n",
    "            \n",
    "        return new_state, reward, is_terminal #keep the same for mat as OpenAI gym.\n",
    "    \n",
    "    def _next_cell(self, state, action) : \n",
    "        \"\"\"Blindly takes the action without checking anything and returns the position\"\"\"\n",
    "        r,c = state #row & column\n",
    "        if action == Exit :\n",
    "            return self.end_state\n",
    "        if action == Up :\n",
    "            target = r-1, c  \n",
    "        if action == Down :\n",
    "            target = r+1, c\n",
    "        if action == Right :\n",
    "            target = r, c+1  \n",
    "        if action == Left :\n",
    "            target = r, c-1 \n",
    "        \n",
    "        if self._valid_cell(target) :\n",
    "            return target\n",
    "        return state #stay put the target is invalid.\n",
    "    \n",
    "    def _valid_cell(self, cell) :\n",
    "        \"\"\"Returns true if the cell is a valid cell\"\"\"\n",
    "        r, c = cell #this may be an illegal node; we need to check\n",
    "        \n",
    "        #is it any of the walls?\n",
    "        if (r,c) in self.walls :\n",
    "            return False\n",
    "        \n",
    "        #is it outside the grid?\n",
    "        if r < 0 or r >= self.rows or c < 0 or c >= self.columns :\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    #pretty print the grid and agent if given.\n",
    "    def print(self, agent_state=None) :\n",
    "        for r in range(self.rows) :\n",
    "            for c in range(self.columns) :\n",
    "                cell = (r,c)\n",
    "                if cell in self.walls :\n",
    "                    print('# ', end='')\n",
    "                elif cell in self.terminals :\n",
    "                    if self.terminals[cell] > 0 :\n",
    "                        print('+', end=' ')\n",
    "                    else :\n",
    "                        print('-', end=' ')\n",
    "                elif cell == agent_state :\n",
    "                    print('@ ', end='')\n",
    "                else :\n",
    "                    print('. ', end='')\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6jw8OLkqtvU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jMlt4WidlLos"
   },
   "outputs": [],
   "source": [
    "grid_world = GridWorld(gamma=0.9, living_reward=-0.04)\n",
    "start = (2,0) #as in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "EoZp-RtKlQEB",
    "outputId": "3fc0f098-160d-4b48-ee21-48e82bf0eaea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . + \n",
      ". # . - \n",
      "@ . . . \n"
     ]
    }
   ],
   "source": [
    "# + and - are the terminal states. @ is our agent.\n",
    "grid_world.print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiIB6IdtlHJj"
   },
   "outputs": [],
   "source": [
    "#This is a simple class to hold the policy dictionary\n",
    "#useful for printing the policy and hiding some details.\n",
    "\n",
    "class Policy :\n",
    "    def __init__(self, grid_world=None) :\n",
    "        \"\"\"Holds one policy and returns actions according to it\"\"\"\n",
    "        self.grid_world = grid_world\n",
    "        self.policy     = { } #{ state: policy_action}\n",
    "        \n",
    "    def __getitem__(self, state) :\n",
    "        return self.policy[state]\n",
    "    \n",
    "    def __setitem__(self, state, action) :\n",
    "        self.policy[state] = action\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Just a pretty print function for easy debugging\n",
    "    def print(self) :\n",
    "        print_chars = {Up:'^', Down:'v', Right:'>', Left:'<', Exit:'+'}\n",
    "        for state in [(r,c) for r in range(self.grid_world.rows) for c in range(self.grid_world.columns)]:\n",
    "            \n",
    "            if state in self.grid_world.terminals :\n",
    "                if self.grid_world.terminals[state] >= 0 :\n",
    "                    print('+', end=' ') #positive reward terminal\n",
    "                else :\n",
    "                    print('-', end=' ') #-ve reward terminal\n",
    "                    \n",
    "            elif state not in self.policy :\n",
    "                print('#', end=' ') #walls\n",
    "            else :\n",
    "                print(print_chars[self.policy[state]], end=' ') #directions >, <, ^, v\n",
    "                \n",
    "            if (state[1]+1) % self.grid_world.columns == 0 :\n",
    "                print(\"\") #just a newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RkfY0MkRCP62"
   },
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "# Now we implement some algorithms \n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sQWER6ecCQGK"
   },
   "outputs": [],
   "source": [
    "def qvalue(grid_world, state, action, V) :\n",
    "    \"\"\"returns the Q value of the state action pair\"\"\"\n",
    "    #  SUM [  P(s' | s, a) * ( R(s,a,s') + V(s2) ) ] of all s' from (s,a)\n",
    "    next_states, actions, p = grid_world.transitions(state, action) \n",
    "    gamma = grid_world.gamma\n",
    "    \n",
    "    values = [ p[i] * ( grid_world.reward(state, actions[i], s) + gamma*V[s] ) \n",
    "              for i,s in enumerate(next_states) ]\n",
    "    #print(values)\n",
    "    #print(sum(values))\n",
    "    return sum( values )\n",
    "\n",
    "def max_qvalue(grid_world, state, V) :\n",
    "    \"\"\"returns the maximum of q values and its action\"\"\"\n",
    "    q = [ (qvalue(grid_world, state, action, V), action) for action in grid_world.actions(state) ]\n",
    "    #print(q)\n",
    "    return max(q) #returns (value, action)\n",
    "\n",
    "def value_iteration(grid_world, N=1000) :\n",
    "    states = grid_world.all_states\n",
    "    #epsilon = 0.0001 * (1-gamma)/gamma\n",
    "    epsilon = 1e-10\n",
    "    \n",
    "    #initialize to 0\n",
    "    #U = { s: 0 for s in states }\n",
    "    V = { s: 0 for s in states }\n",
    "            \n",
    "    while True :\n",
    "        #we keep tracof the maximum value change\n",
    "        #if the maximum value change is less than a small value, epsilon, we can stop our iterations\n",
    "        max_delta = 0 \n",
    "        \n",
    "        for state in states :\n",
    "            if state != grid_world.end_state :\n",
    "                \n",
    "                qmax, qaction = max_qvalue(grid_world, state, V)\n",
    "\n",
    "                delta = abs(qmax - V[state])\n",
    "                max_delta = max( [ max_delta, delta] )#we keep them max of these tow values\n",
    "\n",
    "                #update the Values\n",
    "                V[state] = qmax\n",
    "        \n",
    "        #print(max_delta)\n",
    "        if max_delta < epsilon : #we are not improving much. Converged?\n",
    "            break\n",
    "            \n",
    "    return V\n",
    " \n",
    "def policy_from_value(grid_world, V) :\n",
    "    p = Policy(grid_world)\n",
    "    for state in grid_world.all_states :\n",
    "        if state != grid_world.end_state : #we dont have a policy for this state because no actions are valid\n",
    "            qmax, qaction = max_qvalue(grid_world, state, V)\n",
    "            p[state] = qaction\n",
    "    return p\n",
    "\n",
    "\n",
    "#example run\n",
    "# grid_world = GridWorld(gamma=0.9, living_reward=-0.04)\n",
    "# V = value_iteration(grid_world)\n",
    "# p = policy_from_value(grid_world, V)\n",
    "# p.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGiwdj6RCQaT"
   },
   "outputs": [],
   "source": [
    "#ref: http://incompleteideas.net/book/first/ebook/node43.html\n",
    "\n",
    "def policy_evaluation( grid_world, policy ) :\n",
    "    \"\"\"This will run value iteration until convergence and return the converged Values\"\"\"\n",
    "    states = grid_world.all_states\n",
    "    gamma  = grid_world.gamma\n",
    "    #epsilon = 0.0001 * (1-gamma)/gamma\n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    V = { s: 0 for s in states }\n",
    "            \n",
    "    while True : #we exit when less than epsilon diff is made\n",
    "        max_delta = 0\n",
    "        \n",
    "        for state in states :            \n",
    "            if state != grid_world.end_state : #we dont have a policy for this state because no actions are valid\n",
    "                \n",
    "                action = policy[state] #we run this policy            \n",
    "                q      = qvalue(grid_world, state, action, V)           \n",
    "                #print(q)\n",
    "\n",
    "                delta = abs(q - V[state])\n",
    "                max_delta = max( [ max_delta, delta] )#we keep them max of these tow values\n",
    "\n",
    "                V[state] = q\n",
    "            \n",
    "        if max_delta < epsilon : #we are not improving much. Converged?\n",
    "            break\n",
    "                \n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_improvement(grid_world, policy) :\n",
    "    \"\"\"Returns the new improved policy\"\"\"\n",
    "    \n",
    "    while True :\n",
    "        improving = False\n",
    "        \n",
    "        #find the values for this policy\n",
    "        V = policy_evaluation( grid_world, policy )\n",
    "        \n",
    "        #find the policy according to the new values we got\n",
    "        new_policy = policy_from_value(grid_world, V)\n",
    "    \n",
    "        for state in grid_world.all_states :\n",
    "            if state != grid_world.end_state : #we dont have a policy for this state because no actions are valid\n",
    "\n",
    "                if policy[state] != new_policy[state] : #Do we have an improvement?\n",
    "                    improving = True\n",
    "                \n",
    "        if not improving:\n",
    "            return policy\n",
    "            break\n",
    "            \n",
    "        policy = new_policy\n",
    "        \n",
    "        #for debug\n",
    "        #print('_______')\n",
    "        #policy.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8uac7g9lb46"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "###### Some test code. ########################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fh2ZfGg2logH"
   },
   "outputs": [],
   "source": [
    "def random_policy(grid_world) :\n",
    "    \n",
    "    #we need to choose a random action every time the policy is accessed\n",
    "    #here we overload the getitem \n",
    "    #when the user says policy[state] they get a random action\n",
    "    class _RandomPolicy(Policy) :\n",
    "        def __getitem__(self, state) :\n",
    "            return random.choice(grid_world.actions(state))\n",
    "    \n",
    "    p = _RandomPolicy(grid_world) \n",
    "    return p\n",
    "\n",
    "def fixed_policy(grid_world) :\n",
    "    p = Policy(grid_world)\n",
    "    p.policy = {state: Up for state in grid_world.all_states if state != grid_world.end_state }\n",
    "    p.policy.update({state:Exit for state in grid_world.terminals})\n",
    "    #print(p.policy)\n",
    "    return p\n",
    "\n",
    "def good_policy(grid_world) :\n",
    "    p = Policy(grid_world)\n",
    "    p.policy = {\n",
    "        (0,0):Right, (0,1): Right, (0,2): Right, (0,3) : Exit,\n",
    "        (1,0):Up,    (1,1): Right, (1,2): Up,    (1,3) : Exit,\n",
    "        (2,0):Up,    (2,1): Left,  (2,2): Up,    (2,3) : Left,\n",
    "               }\n",
    "    p.policy.update({state:Exit for state in grid_world.terminals})\n",
    "    #print(p.policy)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rSciC2EJ-uXL"
   },
   "outputs": [],
   "source": [
    "def run(grid_world, state, policy=None) :\n",
    "    \"\"\"runs a full episode and return the total reward\"\"\"\n",
    "    rewards = []\n",
    "    gamma = grid_world.gamma\n",
    "    \n",
    "    time_step = 0\n",
    "    while True :\n",
    "        action = policy[state]\n",
    "        #a.print()\n",
    "        #print(action)\n",
    "        state, r, exited = grid_world.move(state, action)\n",
    "        rewards.append(r * (gamma**time_step) ) #the further we go down, the less we value the reward\n",
    "        if exited :\n",
    "            break    \n",
    "        time_step += 1\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def expected_utility(grid_world, state, policy, N=100) :\n",
    "    \"\"\"run the policy till completion several times and return the expected utility\"\"\"\n",
    "    s = 0.0\n",
    "    for _ in range(N) :\n",
    "        #from the same start state we run till completion, N times\n",
    "        s += sum( run(grid_world, state, policy) )\n",
    "    return s/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "Te1GnYPLlpNz",
    "outputId": "157253fc-a83e-4a13-dd83-271746c2fa7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> > > + \n",
      "^ > ^ - \n",
      "^ < ^ < \n",
      "0.8080799999999979\n",
      "0.8653999999999902\n",
      "0.9268400000000074\n",
      "1.0\n",
      "0.759519999999999\n",
      "0.6283599999999968\n",
      "-1.0\n",
      "0.6965199999999975\n",
      "0.6495200000000001\n",
      "0.6183599999999972\n",
      "0.31216\n"
     ]
    }
   ],
   "source": [
    "#page  651; AIMA Book\n",
    "#The utilities of the states in the 4 × 3 world, calculated with γ = 1 and\n",
    "#R(s) = − 0.04 for nonterminal states\n",
    "\n",
    "\n",
    "\n",
    "N = 1000\n",
    "\n",
    "grid_world = GridWorld(gamma=1.0, living_reward=-0.04)\n",
    "policy = good_policy(grid_world)\n",
    "policy.print()\n",
    "\n",
    "for state in grid_world.all_states :\n",
    "    if state != grid_world.end_state :\n",
    "        print( expected_utility(grid_world, state, policy, N) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRky_dO4DEp6"
   },
   "outputs": [],
   "source": [
    "##### Lets run some value iteration and check the policy from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "18mKvMgytNh6"
   },
   "outputs": [],
   "source": [
    "grid_world = GridWorld(gamma=0.9, living_reward=-0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oNafgb8NtR2o"
   },
   "outputs": [],
   "source": [
    "V = value_iteration(grid_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Li5DOvh4tULe"
   },
   "outputs": [],
   "source": [
    "p = policy_from_value(grid_world, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "gLWHT7wHuHPk",
    "outputId": "cc405e68-e064-41f1-f182-f7d72ef019dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> > > + \n",
      "^ # ^ - \n",
      "^ > ^ < \n"
     ]
    }
   ],
   "source": [
    "p.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PDc3BXIluIUP"
   },
   "outputs": [],
   "source": [
    "####### try policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "RphinsleAMwB",
    "outputId": "b5c84e67-4fd9-4bef-c5dc-2f1bdb369de8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^ ^ ^ + \n",
      "^ # ^ - \n",
      "^ ^ ^ ^ \n",
      "_____\n",
      "> > > + \n",
      "^ # ^ - \n",
      "^ > ^ v \n"
     ]
    }
   ],
   "source": [
    "\n",
    "grid_world = GridWorld(gamma=0.5, living_reward=-0.05)\n",
    "\n",
    "#Before policy\n",
    "p = fixed_policy(grid_world)\n",
    "\n",
    "#new policy\n",
    "newp = policy_improvement(grid_world, p)\n",
    "\n",
    "\n",
    "p.print()\n",
    "print('_____')\n",
    "newp.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vYBNb0mvAPp-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): 0.509415595402796, (0, 1): 0.6495863596130941, (0, 2): 0.7953622428927024, (0, 3): 1.0, (1, 0): 0.39851125448592006, (1, 2): 0.48644045591510504, (1, 3): -1.0, (2, 0): 0.29646654106657166, (2, 1): 0.2539605460893563, (2, 2): 0.34478839971625713, (2, 3): 0.1299424701050445, (-1, -1): 0}\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allStates=grid_world.all_states\n",
    "V[(0,1)]\n",
    "grid_world.actions((1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . + \n",
      ". # . - \n",
      ". . . . \n"
     ]
    }
   ],
   "source": [
    "#grid_world.transitions()\n",
    "grid_world.print() # print the grid world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_world.reward((0,3),grid_world.actions((0,3))[0]) #get reword for state (0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_world.reward((0,1),grid_world.actions((0,1))[0],(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_world.reward((0,1),grid_world.actions((0,1))[0],(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_world.actions((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, 1), (0, 0), (0, 2)], [0, 3, 2], [0.8, 0.1, 0.1])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_world.transitions((0,1),grid_world.actions((0,1))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 0.1, 0.1]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns prob distribution of action resulting from state (0,1)\n",
    "grid_world.transitions((0,1),grid_world.actions((0,1))[0])[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeQ(gridWorld,state,action, valueOfStates):\n",
    "    reward = grid_world.reward(state,action)\n",
    "    nextStates, _, probability = grid_world.transitions(state,action) \n",
    "    val=[probability[i]*(reward+grid_world.gamma*valueOfStates[nextState]) for i, nextState in enumerate(nextStates)]\n",
    "    return sum(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_mine={}\n",
    "for key in grid_world.all_states: #initilize value of all states to zero\n",
    "    V_mine[key]=0\n",
    "\n",
    "for state in grid_world.all_states:\n",
    "    for _ in range(100): # usually we should use a dalta >0 for convergence\n",
    "        actionsOfState = grid_world.actions(state) # all the possible actions of a state\n",
    "        Q_ofActs=[]\n",
    "        for act in actionsOfState: #gonna pick max of all acts\n",
    "            Q = computeQ(grid_world, state, act, V_mine)\n",
    "            Q_ofActs.append(Q)\n",
    "            #print (act, ':', Q_ofActs)\n",
    "        if not Q_ofActs: #empty Q_ofActs, in state (-1,-1), do nothing\n",
    "            pass\n",
    "        else: #update V_mine if Q_ofActs not empty\n",
    "            V_mine[state]=max(Q_ofActs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): -0.05263157894736843,\n",
       " (0, 1): -0.055555555555555566,\n",
       " (0, 2): -0.05263157894736843,\n",
       " (0, 3): 1.0,\n",
       " (1, 0): -0.055555555555555566,\n",
       " (1, 2): -0.05263157894736843,\n",
       " (1, 3): -1.0,\n",
       " (2, 0): -0.055555555555555566,\n",
       " (2, 1): -0.055555555555555566,\n",
       " (2, 2): -0.0554016620498615,\n",
       " (2, 3): -0.09594560564089652,\n",
       " (-1, -1): 0}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.509415595402796,\n",
       " (0, 1): 0.6495863596130941,\n",
       " (0, 2): 0.7953622428927024,\n",
       " (0, 3): 1.0,\n",
       " (1, 0): 0.39851125448592006,\n",
       " (1, 2): 0.48644045591510504,\n",
       " (1, 3): -1.0,\n",
       " (2, 0): 0.29646654106657166,\n",
       " (2, 1): 0.2539605460893563,\n",
       " (2, 2): 0.34478839971625713,\n",
       " (2, 3): 0.1299424701050445,\n",
       " (-1, -1): 0}"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "mdp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
